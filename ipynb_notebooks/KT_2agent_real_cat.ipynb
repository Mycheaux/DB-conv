{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tSrCkPBVRQa"
      },
      "source": [
        "# All Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lgtf27mtqnw"
      },
      "outputs": [],
      "source": [
        "! python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RghktDdEQ4WL"
      },
      "outputs": [],
      "source": [
        "pip install -qqq lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjdGM8B95vlo"
      },
      "outputs": [],
      "source": [
        " ! pip install -qqq ripser kmapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Lt_DVo-enJ"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOXeSxLCbbwh"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWe17EIXRlqq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "print (\"Numpy:\",np.__version__ , \"Scipy:\",sp.__version__, \"Pandas\",pd.__version__)\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import datasets, layers, Sequential, Model, activations, initializers, regularizers, constraints\n",
        "# from tensorflow.keras.layers import InputSpec\n",
        "# import tensorflow.keras.backend as K\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "# print (\"TensorFlow:\",tf.__version__)\n",
        "# # import horovod.tensorflow as hvd\n",
        "# print (\"GPU List:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "import torch\n",
        "# import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "# from pytorch_lightning.loggers import MLFlowLogger\n",
        "# from pytorch_lightning.loggers import NeptuneLogger\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# from pytorch_lightning.plugins import CheckpointIO\n",
        "# from pytorch_lightning.strategies import SingleDeviceStrategy\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.cuda as cuda\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import cross_val_score\n",
        "print (\"Pytorch Normal:\", torch.__version__)\n",
        "print (\"Pytorch Lightning:\", pl.__version__)\n",
        "\n",
        "# import pyspark as ps\n",
        "# from pyspark.mllib.stat import Statistics\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# from mlflow import log_metric, log_param, log_artifacts\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "# import matplotlib.cm as cm\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.datasets import make_friedman1\n",
        "# from sklearn.datasets import make_friedman2\n",
        "# from sklearn.datasets import make_friedman3\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.utils import resample\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import normalize\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import cross_validate\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from math import *\n",
        "\n",
        "import umap\n",
        "\n",
        "from ripser import ripser\n",
        "from ripser import Rips\n",
        "from persim import plot_diagrams\n",
        "# import ripserplusplus as rpp\n",
        "import kmapper as km\n",
        "\n",
        "# import cv2\n",
        "# import PIL\n",
        "# import imageio\n",
        "# import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ozycMRUSbco",
        "outputId": "54a5773f-27b8-432e-88f1-ab0011e60c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvxw1_DjJ3cf"
      },
      "source": [
        "# Useful Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqtPjFMq6hZN"
      },
      "outputs": [],
      "source": [
        "def havrda_charvat_entropy(predictions, labels, parameter_a):\n",
        "    \"\"\"\n",
        "    Calculate the Havrda-Charvat entropy between predicted values and labels.\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray or torch.Tensor): Predicted values (shape: [batch_size, num_classes]).\n",
        "        labels (np.ndarray or torch.Tensor): Ground truth labels (shape: [batch_size, num_classes]).\n",
        "        parameter_a (float, optional): Parameter for the Havrda-Charvat entropy. Default is 2.0.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Havrda-Charvat entropy across the batch.\n",
        "    \"\"\"\n",
        "    assert predictions.shape == labels.shape, \"Predictions and labels must have the same shape.\"\n",
        "\n",
        "    # Normalize predictions and labels to probabilities\n",
        "    predictions_prob = np.clip(predictions.detach().cpu().numpy(), 1e-10, 1.0 - 1e-10)\n",
        "    labels_prob = np.clip(labels.detach().cpu().numpy(), 1e-10, 1.0 - 1e-10)\n",
        "\n",
        "    # Calculate entropy term for each class\n",
        "    entropy_term = -np.sum(predictions_prob * np.log(labels_prob), axis=1)\n",
        "\n",
        "    # Compute the overall Havrda-Charvat entropy\n",
        "    hc_entropy = np.mean((1.0 - np.exp(-parameter_a * entropy_term)) / parameter_a)\n",
        "\n",
        "    return hc_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOMs96HCJzLj"
      },
      "outputs": [],
      "source": [
        "def calc_dist(arr_a,arr_b, method:str):\n",
        "\n",
        "    if method == \"L1\":\n",
        "        distance = np.sum(np.absolute(arr_a - arr_b))\n",
        "    if method == \"L2\":\n",
        "        distance = (np.sum(np.square(arr_a-arr_b)))**0.5\n",
        "    return distance\n",
        "\n",
        "def k_twin(matrix_a:np.array,matrix_b:np.array, k:int, method:str):\n",
        "\n",
        "\n",
        "    n_a = matrix_a.shape[0]\n",
        "    n_b = matrix_b.shape[0]\n",
        "\n",
        "    if n_a != n_b:\n",
        "        print (\"matrix dimention doesn't match\")\n",
        "        return\n",
        "\n",
        "    dist_matrix_a = np.zeros((n_a, n_a))\n",
        "    dist_matrix_b = np.zeros((n_b, n_b))\n",
        "\n",
        "    for i in range (0,n_a):\n",
        "        for j in range (0,n_a):\n",
        "            dist_matrix_a[i,j] = calc_dist(matrix_a[i],matrix_a[j], method)\n",
        "            np.fill_diagonal(dist_matrix_a, np.inf)\n",
        "    # print (dist_matrix_a)\n",
        "\n",
        "    for i in range (0,n_a):\n",
        "        for j in range (0,n_a):\n",
        "            dist_matrix_b[i,j] = calc_dist(matrix_b[i],matrix_b[j], method)\n",
        "            np.fill_diagonal(dist_matrix_b, np.inf)\n",
        "    # print (dist_matrix_b)\n",
        "\n",
        "    k_closest_a = np.zeros((n_a,k))\n",
        "    for i in range (0,n_a):\n",
        "        k_closest_a[i] = np.argsort(dist_matrix_a[i])[:k]\n",
        "    # print(k_closest_a)\n",
        "\n",
        "    k_closest_b = np.zeros((n_b,k))\n",
        "    for i in range (0,n_b):\n",
        "        k_closest_b[i] = np.argsort(dist_matrix_b[i])[:k]\n",
        "    # print(k_closest_b)\n",
        "\n",
        "    total_count = 0\n",
        "    for i in range (0, n_a ):\n",
        "        # print(set(k_closest_a[i]))\n",
        "        count = len(set(k_closest_a[i]).intersection(set(k_closest_b[i])))\n",
        "        total_count += count\n",
        "        # print(total_count)\n",
        "\n",
        "    JI = (total_count/(n_a*k))\n",
        "\n",
        "    return JI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r_wQbrvKC7P"
      },
      "outputs": [],
      "source": [
        "def Category_blind_MH(matrix, boundaries, fold:int):\n",
        "    # Split the matrix column-wise at the given boundaries\n",
        "    linkage_blocks = np.split(matrix, boundaries, axis=1)\n",
        "\n",
        "    # Initialize a list to store the new splits\n",
        "    whole_matrix = []\n",
        "\n",
        "    for block in linkage_blocks:\n",
        "        long_block = []\n",
        "        for i in range (0,fold-1):\n",
        "          block_new = block.copy()\n",
        "          np.random.shuffle(block_new)\n",
        "          long_block.append(block_new)\n",
        "\n",
        "        long_block_final = np.vstack(long_block)\n",
        "        whole_matrix.append(long_block_final)\n",
        "\n",
        "    # Join the splits back into one matrix\n",
        "    final_matrix = np.hstack(whole_matrix)\n",
        "    final_matrix = np.vstack([matrix, final_matrix])\n",
        "\n",
        "    return final_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGNoCKxzVM95"
      },
      "source": [
        "# Data and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRCBFxpUUHeQ"
      },
      "outputs": [],
      "source": [
        "# data_path =  /content/drive/MyDrive/KT/Data\n",
        "x_train = np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/DEAS/D_train_2_2.npy\")\n",
        "x_val = np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/DEAS/D_val_2_2.npy\")\n",
        "x_test = np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/DEAS/D_test_2_2.npy\")\n",
        "\n",
        "y_train= np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/PSEUDO_SHARE/S_train_2_2.npy\")\n",
        "y_val = np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/PSEUDO_SHARE/S_val_2_2.npy\")\n",
        "y_test = np.load(\"/content/drive/MyDrive/KT/Data/KTR_2_2/PSEUDO_SHARE/S_test_2_2.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j21sRw0yroim"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_YLyo-wVBz6",
        "outputId": "e33a13d0-5e35-4b63-ba07-b18751c91926"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cuda.is_available(),cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqViJ-tbeNmF"
      },
      "outputs": [],
      "source": [
        "# m0t0 is general without data augmentation\n",
        "grad_clip = None\n",
        "drop_out_rate = 0.0\n",
        "num_epochs = 5000\n",
        "batch_size = 256\n",
        "l1_lamb = 0\n",
        "l2_lamb = 1\n",
        "pt_lamb = 0\n",
        "hce_lamb = 0\n",
        "tji_lamb = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCAbscTyG7nS"
      },
      "outputs": [],
      "source": [
        "lr_m = 0.0002\n",
        "b1_m = 0.5\n",
        "b2_m = 0.999\n",
        "lr_i = 0.0002\n",
        "b1_i = 0.5\n",
        "b2_i = 0.999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCErMOaosVbK"
      },
      "outputs": [],
      "source": [
        "Mapper_input_size,Mapper_output_size,Mapper_learning_rate = 56,31,1e-3\n",
        "Inverter_input_size,Inverter_output_size,Inverter_learning_rate = 31,56,1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmSLpaXVKeG"
      },
      "outputs": [],
      "source": [
        "x_train_tt = torch.tensor(x_train,dtype=torch.float)#.unsqueeze(1)\n",
        "x_val_tt = torch.tensor(x_val, dtype=torch.float)#.unsqueeze(1)\n",
        "x_test_tt = torch.tensor(x_test, dtype=torch.float)#.unsqueeze(1)\n",
        "y_train_tt = torch.tensor(np.array(y_train),dtype=torch.float)#.unsqueeze(1)\n",
        "y_val_tt = torch.tensor(np.array(y_val), dtype=torch.float)#.unsqueeze(1)\n",
        "y_test_tt = torch.tensor(np.array(y_test), dtype=torch.float)#.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRdHABU3VMKY"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train_tt, y_train_tt)\n",
        "val_dataset = TensorDataset(x_val_tt, y_val_tt)\n",
        "test_dataset = TensorDataset(x_test_tt,y_test_tt)\n",
        "training_loader = DataLoader(dataset= train_dataset, batch_size=batch_size, shuffle= True)\n",
        "validation_loader = DataLoader(dataset= val_dataset, batch_size=batch_size, shuffle= False)\n",
        "test_loader = DataLoader(dataset= test_dataset, batch_size=batch_size, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP_FcLqsY7BU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUPRKNMdV5M6"
      },
      "source": [
        "# Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXTmXhRSV39_"
      },
      "outputs": [],
      "source": [
        "class Mapper (pl.LightningModule):\n",
        "    def __init__(self, Mapper_input_size,Mapper_output_size,Mapper_learning_rate ):\n",
        "        super(Mapper, self).__init__()\n",
        "        self.Mapper_input_size = Mapper_input_size\n",
        "        self.Mapper_output_size = Mapper_output_size\n",
        "\n",
        "        # self.latent_size = latent_size\n",
        "        self.Mapper_learning_rate = Mapper_learning_rate\n",
        "        self.fc1 = nn.Linear(56, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, 128)\n",
        "        self.fc6= nn.Linear(128,31)\n",
        "        # self.fc4= nn.Linear(64,9)\n",
        "        # self.fc3 = nn.Linear(128, 32)\n",
        "        # self.fc4 = nn.Linear(32, 4)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=256, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=512, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn4 = nn.BatchNorm1d(num_features=256, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn5 = nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn6 = nn.BatchNorm1d(num_features=31, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "\n",
        "        self.prelu1 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu2 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu3 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu4 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu5 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu6 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "\n",
        "\n",
        "        # self.softmax = nn.Softmax(dim=-1)\n",
        "        # self.tanh = nn.Tanh()\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x= torch.flatten(x, start_dim=1)\n",
        "        # print (x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.prelu1(x)\n",
        "        # x= self.drop(x)\n",
        "        x= self.bn1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.prelu2(x)\n",
        "        # x= self.drop(x)\n",
        "        x= self.bn2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.prelu3(x)\n",
        "        # x= self.drop(x)\n",
        "        x= self.bn3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.prelu4(x)\n",
        "        # x= self.drop(x)\n",
        "        x= self.bn4(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.prelu5(x)\n",
        "        # x= self.drop(x)\n",
        "        x= self.bn5(x)\n",
        "\n",
        "        x = self.fc6(x)\n",
        "        x = self.prelu6(x)\n",
        "        # x= self.drop(x)\n",
        "        # x= self.bn6(x)\n",
        "\n",
        "\n",
        "        # x = self.fc4(x)\n",
        "        # x = self.softmax(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # x = self.tanh(x)\n",
        "\n",
        "        return (x)\n",
        "\n",
        "class Inverter (pl.LightningModule):\n",
        "    def __init__(self, Inverter_input_size,Inverter_output_size,Inverter_learning_rate ):\n",
        "        super(Inverter, self).__init__()\n",
        "        self.Inverter_input_size = Inverter_input_size\n",
        "        self.Inverter_output_size = Inverter_output_size\n",
        "\n",
        "        # self.latent_size = latent_size\n",
        "        self.Inverter_learning_rate = Inverter_learning_rate\n",
        "        self.fc1 = nn.Linear(31, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        # self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(256, 256)\n",
        "        self.fc5 = nn.Linear(256, 128)\n",
        "        self.fc6= nn.Linear(128,56)\n",
        "        # self.fc4= nn.Linear(64,9)\n",
        "        # self.fc3 = nn.Linear(128, 32)\n",
        "        # self.fc4 = nn.Linear(32, 4)\n",
        "        self.drop = nn.Dropout(p=drop_out_rate)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=256, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=512, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn4 = nn.BatchNorm1d(num_features=256, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn5 = nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "        self.bn6 = nn.BatchNorm1d(num_features=31, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True )\n",
        "\n",
        "        self.prelu1 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu2 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu3 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu4 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu5 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "        self.prelu6 = nn.PReLU(num_parameters=1, init=0.25)\n",
        "\n",
        "\n",
        "        # self.softmax = nn.Softmax(dim=-1)\n",
        "        # self.tanh = nn.Tanh()\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x= torch.flatten(x, start_dim=1)\n",
        "        # print (x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.prelu1(x)\n",
        "        x= self.drop(x)\n",
        "        x= self.bn1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.prelu2(x)\n",
        "        x= self.drop(x)\n",
        "        x= self.bn2(x)\n",
        "\n",
        "        # x = self.fc3(x)\n",
        "        # x = self.prelu3(x)\n",
        "        # x= self.drop(x)\n",
        "        # x= self.bn3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.prelu4(x)\n",
        "        x= self.drop(x)\n",
        "        x= self.bn4(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.prelu5(x)\n",
        "        x= self.drop(x)\n",
        "        x= self.bn5(x)\n",
        "\n",
        "        x = self.fc6(x)\n",
        "        x = self.prelu6(x)\n",
        "        # x= self.drop(x)\n",
        "        # x= self.bn6(x)\n",
        "\n",
        "\n",
        "        # x = self.fc4(x)\n",
        "        # x = self.softmax(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # x = self.tanh(x)\n",
        "\n",
        "        return (x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mnEZk7dsCnC"
      },
      "outputs": [],
      "source": [
        "m = Mapper(Mapper_input_size,Mapper_output_size,Mapper_learning_rate)\n",
        "i = Inverter(Inverter_input_size,Inverter_output_size,Inverter_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPvb-bFIqSFE"
      },
      "outputs": [],
      "source": [
        "class RegenrativeModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        lr_m: float = lr_m,\n",
        "        b1_m: float = b1_m,\n",
        "        b2_m: float = b2_m,\n",
        "        lr_i: float = lr_i,\n",
        "        b1_i: float = b1_i,\n",
        "        b2_i: float = b2_i,\n",
        "\n",
        "        batch_size: int = batch_size,\n",
        "        num_epochs: int = num_epochs,\n",
        "        **kwargs,):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "\n",
        "        self.mapper = Mapper(Mapper_input_size, Mapper_output_size,  Mapper_learning_rate)\n",
        "        self.inverter = Inverter(Inverter_input_size, Inverter_output_size,  Inverter_learning_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "      return self.mapper(data)\n",
        "\n",
        "    def mapper_loss(self,hat,true):\n",
        "        norm = torch.sqrt(torch.sum(hat ** 2, -1, keepdim=True))\n",
        "        normalized_emb = hat / norm\n",
        "        similarity = torch.matmul(normalized_emb, normalized_emb.transpose(1, 0))\n",
        "        batch_size = hat.size(0)\n",
        "        pt = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1)) # cos similarity\n",
        "        hce_loss = havrda_charvat_entropy(hat, true, parameter_a=1.3)\n",
        "        l1_loss = F.l1_loss(hat, true)\n",
        "        l2_loss = F.mse_loss(hat, true)\n",
        "        tji = 1- k_twin(hat.detach().cpu().numpy(), true.detach().cpu().numpy(), 5, \"L2\")\n",
        "        loss = l1_lamb * l1_loss + l2_lamb * l2_loss + pt_lamb *pt + hce_lamb * hce_loss + tji_lamb * tji\n",
        "        self.log(\"train/mapping_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/mapping_l1_loss\", l1_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/mapping_l2_loss\", l2_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/mapping_pt_loss\", pt, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/mapping_hce_loss\", hce_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/mapping_tji_loss\", tji, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def inverter_loss(self,hat,true):\n",
        "        norm = torch.sqrt(torch.sum(hat ** 2, -1, keepdim=True))\n",
        "        normalized_emb = hat / norm\n",
        "        similarity = torch.matmul(normalized_emb, normalized_emb.transpose(1, 0))\n",
        "        batch_size = hat.size(0)\n",
        "        pt = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1)) # cos similarity\n",
        "        hce_loss = havrda_charvat_entropy(hat, true, parameter_a=1.3)\n",
        "        l1_loss = F.l1_loss(hat, true)\n",
        "        l2_loss = F.mse_loss(hat, true)\n",
        "        tji = 1 - k_twin(hat.detach().cpu().numpy(), true.detach().cpu().numpy(), 5, \"L2\")\n",
        "        loss = l1_lamb * l1_loss + l2_lamb * l2_loss + pt_lamb *pt + hce_lamb * hce_loss + tji_lamb * tji\n",
        "        self.log(\"train/inverting_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/inverting_l1_loss\", l1_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/inverting_l2_loss\", l2_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/inverting_pt_loss\", pt, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/inverting_hce_loss\", hce_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/inverting_tji_loss\", tji, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        optimizer_m, optimizer_i = self.optimizers()\n",
        "\n",
        "        for i in range (0,2):\n",
        "            #mapper\n",
        "            self.toggle_optimizer(optimizer_m)\n",
        "            data, label = batch\n",
        "            # y_hat = self.mapper(data)\n",
        "            # y_hat_hat = self.inverter(y_hat)\n",
        "            # m_loss = self.mapper_loss(y_hat_hat, data)\n",
        "            x_hat = self.inverter(label)\n",
        "            x_hat_hat = self.mapper(x_hat)\n",
        "            i_loss = self.inverter_loss(x_hat_hat, label)\n",
        "            self.manual_backward(i_loss)\n",
        "            optimizer_m.step()\n",
        "            optimizer_m.zero_grad()\n",
        "            self.untoggle_optimizer(optimizer_m)\n",
        "        for i in range (0,1):\n",
        "            #inverter\n",
        "            self.toggle_optimizer(optimizer_i)\n",
        "            data, label = batch\n",
        "            # x_hat = self.inverter(label)\n",
        "            # x_hat_hat = self.mapper(x_hat)\n",
        "            # i_loss = self.inverter_loss(x_hat_hat, label)\n",
        "            y_hat = self.mapper(data)\n",
        "            y_hat_hat = self.inverter(y_hat)\n",
        "            m_loss = self.mapper_loss(y_hat_hat, data)\n",
        "            self.manual_backward(m_loss)\n",
        "            optimizer_i.step()\n",
        "            optimizer_i.zero_grad()\n",
        "            self.untoggle_optimizer(optimizer_i)\n",
        "\n",
        "        # self.log(\"train/m_loss\", m_loss, on_step=False, on_epoch=True)\n",
        "        # self.log(\"train/i_loss\", i_loss, on_step=False, on_epoch=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, label = batch\n",
        "        y_hat = self.mapper(data)\n",
        "        y_hat_hat = self.inverter(y_hat)\n",
        "        # m_loss = self.mapper_loss(y_hat_hat, data) #dont demethylate the loss definations are defined for training, they log train loss\n",
        "        m_l1_loss = F.l1_loss(y_hat_hat, data)\n",
        "        m_l2_loss = F.mse_loss(y_hat_hat, data)\n",
        "\n",
        "        x_hat = self.inverter(label)\n",
        "        x_hat_hat = self.mapper(x_hat)\n",
        "        # i_loss = self.inverter_loss(x_hat_hat, label)\n",
        "        i_l1_loss = F.l1_loss(x_hat_hat, label)\n",
        "        i_l2_loss = F.mse_loss(x_hat_hat, label)\n",
        "\n",
        "        self.log(\"val/mapping_l1_loss\", m_l1_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/mapping_l2_loss\", m_l2_loss, on_step=False, on_epoch=True)\n",
        "        # self.log(\"val/mapping_loss\", m_loss, on_step=False, on_epoch=True)\n",
        "\n",
        "        self.log(\"val/inverting_l1_loss\", i_l1_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/inverting_l2_loss\", i_l2_loss, on_step=False, on_epoch=True)\n",
        "        # self.log(\"val/inverting_loss\", i_loss, on_step=False, on_epoch=True)\n",
        "\n",
        "        # pca = PCA(n_components=2)\n",
        "        # pca_mox = pca.fit_transform(y_hat.cpu().numpy())\n",
        "        # pca_iomox = pca.fit_transform(y_hat_hat.cpu().numpy())\n",
        "        # pca_ioy = pca.fit_transform(x_hat.cpu().numpy())\n",
        "        # pca_moxioy = pca.fit_transform(x_hat_hat.cpu().numpy())\n",
        "\n",
        "        # plt.close()\n",
        "\n",
        "        # fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "        # axs[0, 0].scatter(pca_mox[:, 0], pca_mox[:, 1],s=1)\n",
        "        # axs[0, 0].set_title('m(X)')\n",
        "        # axs[0, 1].scatter(pca_iomox[:, 0], pca_mox[:, 1],s=1)\n",
        "        # axs[0, 1].set_title('i(m(X))')\n",
        "        # axs[1, 0].scatter(pca_mox[:, 0], pca_mox[:, 1],s=1)\n",
        "        # axs[1, 0].set_title('i(Y)')\n",
        "        # axs[1, 1].scatter(pca_mox[:, 0], pca_mox[:, 1],s=1)\n",
        "        # axs[1, 1].set_title('m(i(Y))')\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_m = self.hparams.lr_m\n",
        "        b1_m = self.hparams.b1_m\n",
        "        b2_m = self.hparams.b2_m\n",
        "        lr_i = self.hparams.lr_i\n",
        "        b1_i = self.hparams.b1_i\n",
        "        b2_i = self.hparams.b2_i\n",
        "\n",
        "        opt_m = torch.optim.Adam(self.mapper.parameters(), lr=lr_m, betas=(b1_m, b2_m))\n",
        "        opt_i = torch.optim.Adam(self.inverter.parameters(), lr=lr_i, betas=(b1_i, b2_i))\n",
        "        return [opt_m, opt_i], []\n",
        "\n",
        "    # def on_validation_epoch_end(self):\n",
        "    #     # data, label = batch\n",
        "    #     # y_hat = self.mapper(data)\n",
        "    #     # y_hat_hat = self.inverter(y_hat)\n",
        "    #     # m_loss = self.mapper_loss(y_hat_hat, data)\n",
        "    #     # x_hat = self.inverter(label)\n",
        "    #     # x_hat_hat = self.mapper(x_hat)\n",
        "\n",
        "    #     # pca = PCA(n_components=2)\n",
        "    #     # pca_mox = pca.fit_transform(y_hat.cpu().numpy())\n",
        "    #     # pca_iomox = pca.fit_transform(y_hat_hat.cpu().numpy())\n",
        "    #     # pca_ioy = pca.fit_transform(x_hat.cpu().numpy())\n",
        "    #     # pca_moxioy = pca.fit_transform(x_hat_hat.cpu().numpy())\n",
        "\n",
        "    #     # fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "    #     # axs[0, 0].scatter(pca_mox[:, 0], pca_mox[:, 1])\n",
        "    #     # axs[0, 0].set_title('m(X)')\n",
        "    #     # axs[0, 0].scatter(pca_iomox[:, 0], pca_mox[:, 1])\n",
        "    #     # axs[0, 0].set_title('i(m(X))')\n",
        "    #     # axs[0, 0].scatter(pca_mox[:, 0], pca_mox[:, 1])\n",
        "    #     # axs[0, 0].set_title('i(Y)')\n",
        "    #     # axs[0, 0].scatter(pca_mox[:, 0], pca_mox[:, 1])\n",
        "    #     # axs[0, 0].set_title('m(i(Y))')\n",
        "    #     print(\"...\")\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        checkpoint[\"hyperparameters\"] = ( num_epochs, batch_size, Mapper_input_size, Mapper_output_size,  Mapper_learning_rate, Inverter_input_size, Inverter_output_size,  Inverter_learning_rate)\n",
        "\n",
        "    def on_load_checkpoint(self, checkpoint):\n",
        "        num_epochs, batch_size, Mapper_input_size, Mapper_output_size,  Mapper_learning_rate, Inverter_input_size, Inverter_output_size,  Inverter_learning_rate = checkpoint[\"hyperparameters\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_atvWlAQUoE"
      },
      "source": [
        "# Traning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYYn3XdZsGWE"
      },
      "outputs": [],
      "source": [
        "# ! mkdir /content/drive/MyDrive/KTI\n",
        "# ! mkdir /content/drive/MyDrive/KTI/Model\n",
        "! mkdir /content/drive/MyDrive/KTI/Model/ktir6_1\n",
        "! mkdir /content/drive/MyDrive/KTI/Model/ktir6_1/m0t0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw6UIG29sbOs"
      },
      "outputs": [],
      "source": [
        "model = RegenrativeModel()\n",
        "\n",
        "wandb_logger = WandbLogger(name='ktir6_1-m0t0',project='KTI')\n",
        "\n",
        "wandb_logger.watch(model, log=\"all\", log_graph=True) # log frequency != num epochs to print loos rather num steps\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(dirpath='/content/drive/MyDrive/KTI/Model/ktir6_1/m0t0',\n",
        "                                      filename='{epoch}-{training_loss:.2f}-{val_loss:.2f}',\n",
        "                                      # monitor=\"valid/val_loss\", mode = \"min\",\n",
        "                                    #   every_n_train_steps=every_n_train_steps,\n",
        "                                    #   every_n_epochs= 25\n",
        "                                      verbose=True,\n",
        "                                      # save_on_train_epoch_end=True,\n",
        "                                      save_last = True,\n",
        "                                      every_n_epochs= 100,\n",
        "                                      save_top_k=-1\n",
        "                                      )\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=num_epochs,\n",
        "                     callbacks=[ checkpoint_callback],\n",
        "                     gradient_clip_val=grad_clip,\n",
        "                     accelerator=\"gpu\", #amp_backend=\"native\")\n",
        "                     val_check_interval=2,\n",
        "                     logger=wandb_logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79prI5E-GJlc"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, train_dataloaders=training_loader, val_dataloaders= validation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXTz-bcNBcR8"
      },
      "outputs": [],
      "source": [
        "# if torch.isinf(x_train_tt).any():\n",
        "#     print(\"Data contains Infs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxLPjHtFBj6b"
      },
      "outputs": [],
      "source": [
        "# if torch.isnan(x_train_tt).any():\n",
        "#     print(\"Data contains NaNs \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7WP1e1fE2z7"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPjL1AxT65NO"
      },
      "source": [
        "## ktri1-0/m0t0/epoch=599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8I4IHXb4s9v"
      },
      "source": [
        "### Chose model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-0SU8tV1UhY"
      },
      "outputs": [],
      "source": [
        "checkpoint_path= \"/content/drive/MyDrive/KTI/Model/ktir6_0/m0t0/epoch=1199-training_loss=0.00-val_loss=0.00.ckpt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vR27-O5q9_"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcEfrwEyWOP9"
      },
      "outputs": [],
      "source": [
        "# Mapper Loading\n",
        "\n",
        "ckpt_kwargs = {\n",
        "    'Mapper_input_size': Mapper_input_size,\n",
        "    'Mapper_output_size': Mapper_output_size,\n",
        "    'Mapper_learning_rate': Mapper_learning_rate,\n",
        "    # 'map_location': torch.device('cpu')  # Add map_location here if needed\n",
        "}\n",
        "# Mapper = Mapper.load_from_checkpoint(checkpoint, **ckpt_kwargs)\n",
        "# Load the checkpoint state_dict\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "state_dict = checkpoint[\"state_dict\"]\n",
        "\n",
        "# Create a new state_dict with modified keys\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith(\"mapper.\"):\n",
        "        new_state_dict[k[len(\"mapper.\"):]] = v  # Remove the \"mapper.\" prefix\n",
        "    else:\n",
        "        # Handle other keys if needed (e.g., optimizer state)\n",
        "        # or raise an error if they are unexpected\n",
        "        pass\n",
        "\n",
        "# Load the modified state_dict into your model\n",
        "m = Mapper(**ckpt_kwargs)  # Assuming Mapper is your model class\n",
        "m.load_state_dict(new_state_dict, strict=False) # strict=False handles missing or extra keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtJ0p4mH1iUs"
      },
      "outputs": [],
      "source": [
        "# Inverter Loading\n",
        "\n",
        "ckpt_kwargs = {\n",
        "    'Inverter_input_size': Inverter_input_size,\n",
        "    'Inverter_output_size': Inverter_output_size,\n",
        "    'Inverter_learning_rate': Inverter_learning_rate,\n",
        "    # 'map_location': torch.device('cpu')  # Add map_location here if needed\n",
        "}\n",
        "# Inverter = Inverter.load_from_checkpoint(checkpoint, **ckpt_kwargs)\n",
        "# Load the checkpoint state_dict\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "state_dict = checkpoint[\"state_dict\"]\n",
        "\n",
        "# Create a new state_dict with modified keys\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith(\"inverter.\"):\n",
        "        new_state_dict[k[len(\"inverter.\"):]] = v  # Remove the \"mapper.\" prefix\n",
        "    else:\n",
        "        # Handle other keys if needed (e.g., optimizer state)\n",
        "        # or raise an error if they are unexpected\n",
        "        pass\n",
        "\n",
        "# Load the modified state_dict into your model\n",
        "i = Inverter(**ckpt_kwargs)  # Assuming Inverter is your model class\n",
        "i.load_state_dict(new_state_dict, strict=False) # strict=False handles missing or extra keys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKzL2DG50Id"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1EWvk7W3Wfv"
      },
      "outputs": [],
      "source": [
        "# (np.sum(np.abs\n",
        "#  (  x_train_tt.detach().numpy() - i(m(x_train_tt)).detach().numpy()   )\n",
        "#  ))/ (x_train.shape[0]*x_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLRGkPh5UEP"
      },
      "outputs": [],
      "source": [
        "# (np.sum(np.abs\n",
        "#  (  x_test_tt.detach().numpy() - i(m(x_test_tt)).detach().numpy()   )\n",
        "#  ))/ (x_test.shape[0]*x_test.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS4jupYhiz5w"
      },
      "outputs": [],
      "source": [
        " y_hat_hat_train = i(m(x_train_tt)).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-8E-POGkG8a"
      },
      "outputs": [],
      "source": [
        "n_components = 3\n",
        "pca = PCA(n_components=n_components)\n",
        "components = pca.fit_transform((x_train))\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    # color=score,\n",
        "    # labels=anno_mouse_2.Sample_ID,\n",
        "    # symbol=anno_mouse_2.Material,\n",
        "    dimensions=range(n_components),\n",
        "    # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        ")\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Heatmap(z=pd.DataFrame(x_train).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "fig1.update_layout(height=500, width=500)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J08nAA4Ebrdt"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(\n",
        "    x=components[:,0],\n",
        "    y=components[:,1],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,  # Big marker size\n",
        "        color='blue'\n",
        "    )\n",
        "))\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    title='Pred-SHARE-oid by ktr0.2-m2t0e1600',\n",
        "    xaxis_title='PC 1',\n",
        "    yaxis_title='PC 2',\n",
        "\n",
        "    height = 600,\n",
        "    width = 750,\n",
        "    plot_bgcolor='white',  # Background color white\n",
        "\n",
        "    xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "    yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        ")\n",
        "\n",
        "# Display the scatter plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP2E6s87jN5c"
      },
      "outputs": [],
      "source": [
        "print(\"norm_L1_error:\", (np.sum(np.abs(x_train - y_hat_hat_train)))/(y_hat_hat_train.shape[0]*y_hat_hat_train.shape[1]) )\n",
        "print(\"norm_L2_error:\", ((np.sum((x_train - y_hat_hat_train)**2))**0.5)/(y_hat_hat_train.shape[0]*y_hat_hat_train.shape[1]) )\n",
        "\n",
        "\n",
        "n_components = 3\n",
        "pca = PCA(n_components=n_components)\n",
        "components = pca.fit_transform((y_hat_hat_train))\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    # color=score,\n",
        "    # labels=anno_mouse_2.Sample_ID,\n",
        "    # symbol=anno_mouse_2.Material,\n",
        "    dimensions=range(n_components),\n",
        "    # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        ")\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Heatmap(z=pd.DataFrame(y_hat_hat_train).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "fig1.update_layout(height=500, width=500)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyWcHcR7i88e"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(\n",
        "    x=components[:,0],\n",
        "    y=components[:,1],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,  # Big marker size\n",
        "        color='blue'\n",
        "    )\n",
        "))\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    title='Pred-SHARE-oid by ktr0.2-m2t0e1600',\n",
        "    xaxis_title='PC 1',\n",
        "    yaxis_title='PC 2',\n",
        "\n",
        "    height = 600,\n",
        "    width = 750,\n",
        "    plot_bgcolor='white',  # Background color white\n",
        "\n",
        "    xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "    yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        ")\n",
        "\n",
        "# Display the scatter plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12zsmbrYlJpV"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxo-awvBlJpa"
      },
      "outputs": [],
      "source": [
        "# (np.sum(np.abs\n",
        "#  (  x_train_tt.detach().numpy() - i(m(x_train_tt)).detach().numpy()   )\n",
        "#  ))/ (x_train.shape[0]*x_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwF4Ya0nlJpc"
      },
      "outputs": [],
      "source": [
        "# (np.sum(np.abs\n",
        "#  (  x_test_tt.detach().numpy() - i(m(x_test_tt)).detach().numpy()   )\n",
        "#  ))/ (x_test.shape[0]*x_test.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mapper"
      ],
      "metadata": {
        "id": "W4p9W6NBiyw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfgqxt0ClJpc"
      },
      "outputs": [],
      "source": [
        " y_hat_hat_test = i(m(x_test_tt)).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhmJs6felJpc"
      },
      "outputs": [],
      "source": [
        "# n_components = 3\n",
        "# pca = PCA(n_components=n_components)\n",
        "# components = pca.fit_transform((x_test))\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     components,\n",
        "#     # color=score,\n",
        "#     # labels=anno_mouse_2.Sample_ID,\n",
        "#     # symbol=anno_mouse_2.Material,\n",
        "#     dimensions=range(n_components),\n",
        "#     # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()\n",
        "# fig1 = go.Figure()\n",
        "# fig1.add_trace(go.Heatmap(z=pd.DataFrame(x_test).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "# fig1.update_layout(height=500, width=500)\n",
        "# fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c947k3wUbWVW"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=components[:,0],\n",
        "#     y=components[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='Pred-SHARE-oid by ktr0.2-m2t0e1600',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scHlsVtClJpd"
      },
      "outputs": [],
      "source": [
        "# print(\"norm_L1_error:\", (np.sum(np.abs(x_test - y_hat_hat_test)))/(y_hat_hat_test.shape[0]*y_hat_hat_test.shape[1]) )\n",
        "# print(\"norm_L2_error:\", ((np.sum((x_test - y_hat_hat_test)**2))**0.5)/(y_hat_hat_test.shape[0]*y_hat_hat_test.shape[1]) )\n",
        "\n",
        "\n",
        "# n_components = 3\n",
        "# pca = PCA(n_components=n_components)\n",
        "# components = pca.fit_transform((y_hat_hat_test))\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     components,\n",
        "#     # color=score,\n",
        "#     # labels=anno_mouse_2.Sample_ID,\n",
        "#     # symbol=anno_mouse_2.Material,\n",
        "#     dimensions=range(n_components),\n",
        "#     # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()\n",
        "# fig1 = go.Figure()\n",
        "# fig1.add_trace(go.Heatmap(z=pd.DataFrame(y_hat_hat_test).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "# fig1.update_layout(height=500, width=500)\n",
        "# fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSNVKOSAABXb"
      },
      "outputs": [],
      "source": [
        "# y_hat_hat_rand_test = np.random.rand(x_test.shape[0], x_test.shape[1])\n",
        "# y_hat_hat_test_norm = (y_hat_hat_rand_test-y_hat_hat_rand_test.min())/(y_hat_hat_rand_test.max()-y_hat_hat_rand_test.min())\n",
        "# print(\"random_norm_L1_error:\", (np.sum(np.abs(x_test - y_hat_hat_rand_test)))/(y_hat_hat_rand_test.shape[0]*y_hat_hat_rand_test.shape[1]) )\n",
        "# print(\"random_norm_L2_error:\", ((np.sum((x_test - y_hat_hat_rand_test)**2))**0.5)/(y_hat_hat_rand_test.shape[0]*y_hat_hat_rand_test.shape[1]) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = x_test\n",
        "predicted = y_hat_hat_test\n",
        "\n",
        "# Choose distance metric ('L1' or 'L2')\n",
        "distance_metric = 'L1'\n",
        "\n",
        "# Function to calculate distance\n",
        "def calculate_distance(a, b, metric):\n",
        "    if metric == 'L1':\n",
        "        return (np.sum(np.abs(a - b)))/(a.shape[0]*a.shape[1])\n",
        "    # elif metric == 'L2':\n",
        "    #     return euclidean(a, b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid distance metric. Choose 'L1' or 'L2'.\")\n",
        "\n",
        "# Calculate distance for the original predicted\n",
        "original_distance = calculate_distance(target, predicted, distance_metric)\n",
        "\n",
        "# Calculate distances for 1000 random shuffles of predicted\n",
        "random_distances = []\n",
        "for _ in range(1000):\n",
        "    rand_prediction = np.random.rand(predicted.shape[0], predicted.shape[1])\n",
        "    rand_prediction_norm = (rand_prediction-rand_prediction.min())/(rand_prediction.max()-rand_prediction.min())\n",
        "\n",
        "    distance = calculate_distance(target, rand_prediction_norm, distance_metric)\n",
        "    random_distances.append(distance)\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = np.sum(np.array(random_distances) <= original_distance) / len(random_distances)\n",
        "\n",
        "print(f\"Original {distance_metric} distance: {original_distance:.2f}\")\n",
        "print(f\"Mean random {distance_metric} distance: {np.mean(random_distances):.2f}\")\n",
        "print(f\"P-value: {p_value:.7f}\")"
      ],
      "metadata": {
        "id": "Tt6T10Jp_KJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = x_test\n",
        "predicted = y_hat_hat_test\n",
        "\n",
        "# Choose distance metric ('L1' or 'L2')\n",
        "distance_metric = 'L1'\n",
        "\n",
        "# Function to calculate distance\n",
        "def calculate_distance(a, b, metric):\n",
        "    if metric == 'L1':\n",
        "        return (np.sum(np.abs(a - b)))/(a.shape[0]*a.shape[1])\n",
        "    # elif metric == 'L2':\n",
        "    #     return euclidean(a, b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid distance metric. Choose 'L1' or 'L2'.\")\n",
        "\n",
        "# Calculate distance for the original predicted\n",
        "original_distance = calculate_distance(target, predicted, distance_metric)\n",
        "\n",
        "# Calculate distances for 1000 random shuffles of predicted\n",
        "random_distances = []\n",
        "\n",
        "shuffled_pred = predicted.copy()\n",
        "for _ in range(1000):\n",
        "    # rand_prediction = np.random.rand(predicted.shape[0], predicted.shape[1])\n",
        "    # rand_prediction_norm = (rand_prediction-rand_prediction.min())/(rand_prediction.max()-rand_prediction.min())\n",
        "    np.random.shuffle(shuffled_pred)\n",
        "\n",
        "    distance = calculate_distance(target, shuffled_pred, distance_metric)\n",
        "    random_distances.append(distance)\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = np.sum(np.array(random_distances) <= original_distance) / len(random_distances)\n",
        "\n",
        "print(f\"Original {distance_metric} distance: {original_distance:.2f}\")\n",
        "print(f\"Mean random {distance_metric} distance: {np.mean(random_distances):.2f}\")\n",
        "print(f\"P-value: {p_value:.7f}\")"
      ],
      "metadata": {
        "id": "kvY6S134_M5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldwZJ12alJpe"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=components[:,0],\n",
        "#     y=components[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='i(m(A_test))',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kZPqxbX_Zap"
      },
      "outputs": [],
      "source": [
        "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
        "# umap_x = umap_model.fit_transform(x_test)\n",
        "# umap_y_hat_hat = umap_model.fit_transform(y_hat_hat_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCpeSImV_bxN"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=umap_x[:,0],\n",
        "#     y=umap_x[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='A_test',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzan5b-F_e4h"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=umap_y_hat_hat[:,0],\n",
        "#     y=umap_y_hat_hat[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='i(m(A_test))',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per = 10\n",
        "k = (x_test.shape[0]*per)//100\n",
        "k"
      ],
      "metadata": {
        "id": "6D3zvuu0ANqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "components_a = x_test\n",
        "components_b = y_hat_hat_test\n",
        "\n",
        "x = (1, 5, 10, 15, 20, 30, 31, 40, 50, 55)\n",
        "y = np.zeros_like(x)\n",
        "for i,x_i in enumerate(x):\n",
        "    # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "    print (\"for \", x_i, \":\", k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ],
      "metadata": {
        "id": "K-iPKmTJ_UC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CLmVNE76e8G"
      },
      "outputs": [],
      "source": [
        "# components_a = x_test\n",
        "# components_b = y_hat_hat_test\n",
        "\n",
        "# x = np.arange (1,20, 10)\n",
        "# y = np.zeros_like(x)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf2Vd-Li_nea"
      },
      "outputs": [],
      "source": [
        "# components_a = umap_x\n",
        "# components_b = umap_y_hat_hat\n",
        "\n",
        "# x = np.arange (1,20, 10)\n",
        "# y = np.zeros_like(x)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPi0m6xFlJpf"
      },
      "outputs": [],
      "source": [
        "# rips = Rips(maxdim=2, thresh= 0.9)\n",
        "# data = x_test\n",
        "# diagrams_b = rips.fit_transform(data)\n",
        "# rips.plot(diagrams_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iId0GgOslJpf"
      },
      "outputs": [],
      "source": [
        "# rips = Rips(maxdim=2, thresh= 0.9)\n",
        "# data = y_hat_hat_test\n",
        "# diagrams_b = rips.fit_transform(data)\n",
        "# rips.plot(diagrams_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "div38B3X7-eE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inverter"
      ],
      "metadata": {
        "id": "lb8rMvyMi3Cv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjM9M0sjjByi"
      },
      "outputs": [],
      "source": [
        "x_hat_hat_test = m(i(y_test_tt)).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMMGBvl5jByi"
      },
      "outputs": [],
      "source": [
        "# n_components = 3\n",
        "# pca = PCA(n_components=n_components)\n",
        "# components = pca.fit_transform((y_test))\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     components,\n",
        "#     # color=score,\n",
        "#     # labels=anno_mouse_2.Sample_ID,\n",
        "#     # symbol=anno_mouse_2.Material,\n",
        "#     dimensions=range(n_components),\n",
        "#     # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()\n",
        "# fig1 = go.Figure()\n",
        "# fig1.add_trace(go.Heatmap(z=pd.DataFrame(x_test).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "# fig1.update_layout(height=500, width=500)\n",
        "# fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY0UPeP0jByj"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=components[:,0],\n",
        "#     y=components[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='Pred-SHARE-oid by ktr0.2-m2t0e1600',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqXFEzRRjByk"
      },
      "outputs": [],
      "source": [
        "# print(\"norm_L1_error:\", (np.sum(np.abs(y_test - x_hat_hat_test)))/(x_hat_hat_test.shape[0]*x_hat_hat_test.shape[1]) )\n",
        "# print(\"norm_L2_error:\", ((np.sum((y_test - x_hat_hat_test)**2))**0.5)/(x_hat_hat_test.shape[0]*x_hat_hat_test.shape[1]) )\n",
        "\n",
        "\n",
        "# n_components = 3\n",
        "# pca = PCA(n_components=n_components)\n",
        "# components = pca.fit_transform((x_hat_hat_test))\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     components,\n",
        "#     # color=score,\n",
        "#     # labels=anno_mouse_2.Sample_ID,\n",
        "#     # symbol=anno_mouse_2.Material,\n",
        "#     dimensions=range(n_components),\n",
        "#     # title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()\n",
        "# fig1 = go.Figure()\n",
        "# fig1.add_trace(go.Heatmap(z=pd.DataFrame(x_hat_hat_test).corr(), colorscale='Puor', showscale=True, zmin=-1, zmax=1,))\n",
        "# fig1.update_layout(height=500, width=500)\n",
        "# fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ_jXEpHjByk"
      },
      "outputs": [],
      "source": [
        "# x_hat_hat_rand_test = np.random.rand(y_test.shape[0], y_test.shape[1])\n",
        "# x_hat_hat_test_norm = (x_hat_hat_rand_test-x_hat_hat_rand_test.min())/(x_hat_hat_rand_test.max()-x_hat_hat_rand_test.min())\n",
        "# print(\"random_norm_L1_error:\", (np.sum(np.abs(y_test - x_hat_hat_rand_test)))/(x_hat_hat_rand_test.shape[0]*x_hat_hat_rand_test.shape[1]) )\n",
        "# print(\"random_norm_L2_error:\", ((np.sum((y_test - x_hat_hat_rand_test)**2))**0.5)/(x_hat_hat_rand_test.shape[0]*x_hat_hat_rand_test.shape[1]) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = y_test\n",
        "predicted = x_hat_hat_test\n",
        "\n",
        "# Choose distance metric ('L1' or 'L2')\n",
        "distance_metric = 'L1'\n",
        "\n",
        "# Function to calculate distance\n",
        "def calculate_distance(a, b, metric):\n",
        "    if metric == 'L1':\n",
        "        return (np.sum(np.abs(a - b)))/(a.shape[0]*a.shape[1])\n",
        "    # elif metric == 'L2':\n",
        "    #     return euclidean(a, b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid distance metric. Choose 'L1' or 'L2'.\")\n",
        "\n",
        "# Calculate distance for the original predicted\n",
        "original_distance = calculate_distance(target, predicted, distance_metric)\n",
        "\n",
        "# Calculate distances for 1000 random shuffles of predicted\n",
        "random_distances = []\n",
        "for _ in range(1000):\n",
        "    rand_prediction = np.random.rand(predicted.shape[0], predicted.shape[1])\n",
        "    rand_prediction_norm = (rand_prediction-rand_prediction.min())/(rand_prediction.max()-rand_prediction.min())\n",
        "\n",
        "    distance = calculate_distance(target, rand_prediction_norm, distance_metric)\n",
        "    random_distances.append(distance)\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = np.sum(np.array(random_distances) <= original_distance) / len(random_distances)\n",
        "\n",
        "print(f\"Original {distance_metric} distance: {original_distance:.2f}\")\n",
        "print(f\"Mean random {distance_metric} distance: {np.mean(random_distances):.2f}\")\n",
        "print(f\"P-value: {p_value:.7f}\")"
      ],
      "metadata": {
        "id": "nsfYCbwcBBgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = y_test\n",
        "predicted = x_hat_hat_test\n",
        "\n",
        "# Choose distance metric ('L1' or 'L2')\n",
        "distance_metric = 'L1'\n",
        "\n",
        "# Function to calculate distance\n",
        "def calculate_distance(a, b, metric):\n",
        "    if metric == 'L1':\n",
        "        return (np.sum(np.abs(a - b)))/(a.shape[0]*a.shape[1])\n",
        "    # elif metric == 'L2':\n",
        "    #     return euclidean(a, b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid distance metric. Choose 'L1' or 'L2'.\")\n",
        "\n",
        "# Calculate distance for the original predicted\n",
        "original_distance = calculate_distance(target, predicted, distance_metric)\n",
        "\n",
        "# Calculate distances for 1000 random shuffles of predicted\n",
        "random_distances = []\n",
        "\n",
        "shuffled_pred = predicted.copy()\n",
        "for _ in range(1000):\n",
        "    # rand_prediction = np.random.rand(predicted.shape[0], predicted.shape[1])\n",
        "    # rand_prediction_norm = (rand_prediction-rand_prediction.min())/(rand_prediction.max()-rand_prediction.min())\n",
        "    np.random.shuffle(shuffled_pred)\n",
        "\n",
        "    distance = calculate_distance(target, shuffled_pred, distance_metric)\n",
        "    random_distances.append(distance)\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = np.sum(np.array(random_distances) <= original_distance) / len(random_distances)\n",
        "\n",
        "print(f\"Original {distance_metric} distance: {original_distance:.2f}\")\n",
        "print(f\"Mean random {distance_metric} distance: {np.mean(random_distances):.2f}\")\n",
        "print(f\"P-value: {p_value:.7f}\")"
      ],
      "metadata": {
        "id": "LIkRVxR5BKCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpP46vFGjByl"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=components[:,0],\n",
        "#     y=components[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='m(i(B_test))',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW0HEqVLjBym"
      },
      "outputs": [],
      "source": [
        "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
        "# umap_x = umap_model.fit_transform(y_test)\n",
        "# umap_y_hat_hat = umap_model.fit_transform(x_hat_hat_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rA5DXhwjByn"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=umap_x[:,0],\n",
        "#     y=umap_x[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='B_test',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od-zecc0jByn"
      },
      "outputs": [],
      "source": [
        "# fig = go.Figure(data=go.Scatter(\n",
        "#     x=umap_y_hat_hat[:,0],\n",
        "#     y=umap_y_hat_hat[:,1],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=5,  # Big marker size\n",
        "#         color='blue'\n",
        "#     )\n",
        "# ))\n",
        "\n",
        "# # Customize the layout\n",
        "# fig.update_layout(\n",
        "#     title='m(i(B_test))',\n",
        "#     xaxis_title='PC 1',\n",
        "#     yaxis_title='PC 2',\n",
        "\n",
        "#     height = 600,\n",
        "#     width = 750,\n",
        "#     plot_bgcolor='white',  # Background color white\n",
        "\n",
        "#     xaxis=dict(showgrid=True, gridcolor='black',zeroline = True, zerolinecolor=\"black\", showline= True, linecolor=\"black\"),\n",
        "#     yaxis=dict(showgrid=True,  gridcolor='black',zeroline = True, zerolinecolor=\"black\",showline= True, linecolor=\"black\"),\n",
        "\n",
        "# )\n",
        "\n",
        "# # Display the scatter plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vSApR6hjByo"
      },
      "outputs": [],
      "source": [
        "# components_a = y_test\n",
        "# components_b = x_hat_hat_test\n",
        "\n",
        "# x = np.arange (1,20, 10)\n",
        "# y = np.zeros_like(x)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEhAgPVbjByo"
      },
      "outputs": [],
      "source": [
        "# components_a = umap_x\n",
        "# components_b = umap_y_hat_hat\n",
        "\n",
        "# x = np.arange (1,20, 10)\n",
        "# y = np.zeros_like(x)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "components_a = y_test\n",
        "components_b = x_hat_hat_test\n",
        "\n",
        "x = (1, 5, 10, 15, 20, 30, 31, 40, 50, 55)\n",
        "y = np.zeros_like(x)\n",
        "for i,x_i in enumerate(x):\n",
        "    # y[i]= k_twin(components_a,components_b, x_i, \"L1\")\n",
        "    print (\"for \", x_i, \":\", k_twin(components_a,components_b, x_i, \"L1\"))\n",
        "\n",
        "# print(\"-----\")\n",
        "\n",
        "# rnd = np.zeros_like(x)\n",
        "# np.random.shuffle(components_b)\n",
        "# for i,x_i in enumerate(x):\n",
        "#     # rnd[i]= k_twin(components_a,components_b, x_i, \"L2\")\n",
        "#     print (k_twin(components_a,components_b, x_i, \"L2\"))"
      ],
      "metadata": {
        "id": "7al3sFSfBZuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGJbCSrWjByp"
      },
      "outputs": [],
      "source": [
        "rips = Rips(maxdim=2, thresh= 0.9)\n",
        "data = x_test\n",
        "diagrams_b = rips.fit_transform(data)\n",
        "rips.plot(diagrams_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouHUt6h-jByp"
      },
      "outputs": [],
      "source": [
        "rips = Rips(maxdim=2, thresh= 0.9)\n",
        "data = y_hat_hat_test\n",
        "diagrams_b = rips.fit_transform(data)\n",
        "rips.plot(diagrams_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhazb9F6jByp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}